{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topik 1:\n",
            "0.005*\"people\" + 0.005*\"year\" + 0.004*\"sir\" + 0.004*\"uk\" + 0.004*\"military\"\n",
            "\n",
            "Topik 2:\n",
            "0.019*\"says\" + 0.009*\"bbc\" + 0.006*\"south\" + 0.006*\"police\" + 0.005*\"government\"\n",
            "\n",
            "Topik 3:\n",
            "0.011*\"new\" + 0.007*\"deal\" + 0.006*\"uk\" + 0.006*\"trump\" + 0.006*\"white\"\n",
            "\n",
            "Topik 4:\n",
            "0.009*\"uk\" + 0.009*\"bbc\" + 0.008*\"england\" + 0.007*\"says\" + 0.006*\"year\"\n",
            "\n",
            "Topik 5:\n",
            "0.008*\"new\" + 0.008*\"bbc\" + 0.007*\"says\" + 0.007*\"people\" + 0.006*\"site\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ====================== 1. Import Library ======================\n",
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora, models\n",
        "\n",
        "# ====================== 2. Load Dataset ======================\n",
        "df = pd.read_csv(\"bbc_cleaned.csv\")\n",
        "\n",
        "# Pastikan kolom 'description' tidak kosong\n",
        "df = df.dropna(subset=['description'])\n",
        "\n",
        "# ====================== 3. Preprocessing ======================\n",
        "def preprocess(text):\n",
        "    return [word for word in simple_preprocess(text, deacc=True) if word not in STOPWORDS]\n",
        "\n",
        "df['tokens'] = df['description'].apply(preprocess)\n",
        "\n",
        "# ====================== 4. Dictionary dan Corpus ======================\n",
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
        "\n",
        "# ====================== 5. LDA Topic Modeling ======================\n",
        "lda_model = models.LdaModel(corpus=corpus,\n",
        "                            id2word=dictionary,\n",
        "                            num_topics=5,         # Ubah jumlah topik jika diperlukan\n",
        "                            random_state=42,\n",
        "                            passes=15,\n",
        "                            alpha='auto',\n",
        "                            per_word_topics=True)\n",
        "\n",
        "# ====================== 6. Tampilkan Topik ======================\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for idx, topic in topics:\n",
        "    print(f\"Topik {idx + 1}:\\n{topic}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Persentase Term untuk Topik 1:\n",
            "people: 22.06%\n",
            "year: 20.49%\n",
            "sir: 19.16%\n",
            "uk: 19.15%\n",
            "military: 19.15%\n",
            "\n",
            "Persentase Term untuk Topik 2:\n",
            "says: 42.24%\n",
            "bbc: 19.50%\n",
            "south: 13.64%\n",
            "police: 12.99%\n",
            "government: 11.63%\n",
            "\n",
            "Persentase Term untuk Topik 3:\n",
            "new: 30.59%\n",
            "deal: 20.78%\n",
            "uk: 16.86%\n",
            "trump: 15.89%\n",
            "white: 15.88%\n",
            "\n",
            "Persentase Term untuk Topik 4:\n",
            "uk: 23.77%\n",
            "bbc: 22.86%\n",
            "england: 21.00%\n",
            "says: 17.77%\n",
            "year: 14.61%\n",
            "\n",
            "Persentase Term untuk Topik 5:\n",
            "new: 22.31%\n",
            "bbc: 21.64%\n",
            "says: 19.54%\n",
            "people: 18.60%\n",
            "site: 17.90%\n"
          ]
        }
      ],
      "source": [
        "# ====================== 7. Hitung Persentase Term per Topik ======================\n",
        "def get_term_percentage(lda_model, corpus, dictionary, num_topics, num_words=5):\n",
        "    # Inisialisasi dictionary untuk menyimpan bobot term per topik\n",
        "    term_weights = {i: {} for i in range(num_topics)}\n",
        "    \n",
        "    # Ambil bobot term dari model LDA\n",
        "    for topic_id in range(num_topics):\n",
        "        topic_terms = lda_model.show_topic(topic_id, topn=num_words)\n",
        "        for term, weight in topic_terms:\n",
        "            term_weights[topic_id][term] = weight\n",
        "    \n",
        "    # Hitung total bobot per topik\n",
        "    total_weights = {i: sum(term_weights[i].values()) for i in range(num_topics)}\n",
        "    \n",
        "    # Hitung persentase kontribusi term per topik\n",
        "    term_percentage = {i: {} for i in range(num_topics)}\n",
        "    for topic_id in range(num_topics):\n",
        "        for term, weight in term_weights[topic_id].items():\n",
        "            term_percentage[topic_id][term] = (weight / total_weights[topic_id]) * 100\n",
        "    \n",
        "    return term_percentage\n",
        "\n",
        "# ====================== 8. Tampilkan Persentase Term ======================\n",
        "term_percentage = get_term_percentage(lda_model, corpus, dictionary, num_topics=5, num_words=5)\n",
        "\n",
        "for topic_id in range(5):\n",
        "    print(f\"\\nPersentase Term untuk Topik {topic_id + 1}:\")\n",
        "    for term, percentage in term_percentage[topic_id].items():\n",
        "        print(f\"{term}: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 218.71\n",
            "Coherence Score: 0.55\n",
            "Log Likelihood: -92893.76\n",
            "Topic Diversity: 0.72\n"
          ]
        }
      ],
      "source": [
        "# ====================== 9. Evaluasi Model LDA ======================\n",
        "from gensim.models import CoherenceModel\n",
        "import numpy as np\n",
        "\n",
        "# 1. Perplexity\n",
        "perplexity = lda_model.log_perplexity(corpus)\n",
        "print(f\"Perplexity: {np.exp2(-perplexity):.2f}\")\n",
        "\n",
        "# 2. Coherence Score\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=df['tokens'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score:.2f}\")\n",
        "\n",
        "# 3. Log Likelihood\n",
        "log_likelihood = lda_model.log_perplexity(corpus) * sum(len(doc) for doc in corpus)\n",
        "print(f\"Log Likelihood: {log_likelihood:.2f}\")\n",
        "\n",
        "# 4. Topic Diversity\n",
        "def topic_diversity(model, topn=10):\n",
        "    topics = model.show_topics(num_topics=model.num_topics, num_words=topn, formatted=False)\n",
        "    all_words = [word for topic in topics for word, _ in topic[1]]\n",
        "    unique_words = set(all_words)\n",
        "    return len(unique_words) / (model.num_topics * topn)\n",
        "\n",
        "diversity = topic_diversity(lda_model)\n",
        "print(f\"Topic Diversity: {diversity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: C:\\Users\\Sabrina\\AppData\\Local/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2wwdJFwINjpCSNa4DknOUB9DfzM_3p6Zi1VWZr323oyzneU41"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "\n",
        "# Konfigurasi halaman Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"Dashboard LDA Topic Modeling\",\n",
        "    page_icon=\"📊\",\n",
        ")\n",
        "\n",
        "# Judul aplikasi\n",
        "st.title(\"LDA Topic Modeling Dashboard\")\n",
        "st.write(\"Analisis topik dari dataset BBC menggunakan model LDA.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"Hasil Analisis Topik\")\n",
        "\n",
        "# ====================== 1. Load dan Persiapkan Data ======================\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    data = pd.read_csv(\"bbc_cleaned.csv\")\n",
        "    data = data.dropna(subset=['description'])\n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "\n",
        "# ====================== 2. Preprocessing dan Model LDA ======================\n",
        "@st.cache_resource\n",
        "def train_lda_model():\n",
        "    # Preprocessing\n",
        "    def preprocess(text):\n",
        "        return [word for word in simple_preprocess(text, deacc=True) if word not in STOPWORDS]\n",
        "\n",
        "    data['tokens'] = data['description'].apply(preprocess)\n",
        "    \n",
        "    # Dictionary dan Corpus\n",
        "    dictionary = corpora.Dictionary(data['tokens'])\n",
        "    corpus = [dictionary.doc2bow(text) for text in data['tokens']]\n",
        "    \n",
        "    # Latih model LDA\n",
        "    lda_model = models.LdaModel(corpus=corpus,\n",
        "                                id2word=dictionary,\n",
        "                                num_topics=5,\n",
        "                                random_state=42,\n",
        "                                passes=15,\n",
        "                                alpha='auto',\n",
        "                                per_word_topics=True)\n",
        "    \n",
        "    # Simpan model (opsional)\n",
        "    joblib.dump(lda_model, 'lda_model.pkl')\n",
        "    joblib.dump(dictionary, 'dictionary.pkl')\n",
        "    return lda_model, dictionary, corpus, data\n",
        "\n",
        "lda_model, dictionary, corpus, data = train_lda_model()\n",
        "\n",
        "# ====================== 3. Fungsi untuk Persentase Term ======================\n",
        "def get_term_percentage(lda_model, num_topics, num_words=5):\n",
        "    term_weights = {i: {} for i in range(num_topics)}\n",
        "    for topic_id in range(num_topics):\n",
        "        topic_terms = lda_model.show_topic(topic_id, topn=num_words)\n",
        "        for term, weight in topic_terms:\n",
        "            term_weights[topic_id][term] = weight\n",
        "    total_weights = {i: sum(term_weights[i].values()) for i in range(num_topics)}\n",
        "    term_percentage = {i: {} for i in range(num_topics)}\n",
        "    for topic_id in range(num_topics):\n",
        "        for term, weight in term_weights[topic_id].items():\n",
        "            term_percentage[topic_id][term] = (weight / total_weights[topic_id]) * 100\n",
        "    return term_percentage\n",
        "\n",
        "term_percentage = get_term_percentage(lda_model, num_topics=5, num_words=5)\n",
        "\n",
        "# ====================== 4. Komponen Dashboard ======================\n",
        "# Dropdown untuk memilih topik\n",
        "st.subheader(\"Pilih Topik\")\n",
        "topic_id = st.selectbox(\"Pilih Topik untuk Ditampilkan:\", [f\"Topik {i+1}\" for i in range(5)], index=0)\n",
        "selected_topic = int(topic_id.split()[1]) - 1\n",
        "\n",
        "# Tampilkan kata kunci topik\n",
        "st.subheader(\"Kata Kunci Topik\")\n",
        "topic_terms = lda_model.show_topic(selected_topic, topn=5)\n",
        "keywords = \", \".join([f\"{term} ({weight:.3f})\" for term, weight in topic_terms])\n",
        "st.write(f\"Kata kunci untuk {topic_id}: {keywords}\")\n",
        "\n",
        "# Tampilkan persentase term\n",
        "st.subheader(\"Persentase Term per Topik\")\n",
        "terms = list(term_percentage[selected_topic].keys())\n",
        "percentages = list(term_percentage[selected_topic].values())\n",
        "for term, percentage in zip(terms, percentages):\n",
        "    st.write(f\"{term}: {percentage:.2f}%\")\n",
        "\n",
        "# Visualisasi persentase term\n",
        "st.subheader(\"Visualisasi Persentase Term\")\n",
        "fig = px.bar(\n",
        "    x=terms,\n",
        "    y=percentages,\n",
        "    labels={'x': 'Term', 'y': 'Persentase (%)'},\n",
        "    title=f'Persentase Term pada {topic_id}',\n",
        "    color=terms,\n",
        "    color_discrete_sequence=px.colors.qualitative.Plotly\n",
        ")\n",
        "fig.update_layout(showlegend=False)\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# ====================== 5. Informasi Tambahan ======================\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"Informasi Dataset\")\n",
        "st.write(f\"Jumlah dokumen: {len(data)}\")\n",
        "st.write(\"Dataset: bbc_cleaned.csv\")\n",
        "st.write(\"Jumlah topik: 5\")\n",
        "st.write(\"Jumlah kata kunci per topik: 5\")\n",
        "\n",
        "# ====================== 6. Evaluasi Model LDA ======================\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"📈 Evaluasi Model LDA\")\n",
        "\n",
        "# Opsi evaluasi\n",
        "with st.expander(\"⚙️ Opsi Evaluasi\"):\n",
        "    coherence_type = st.selectbox(\"Pilih Jenis Coherence\", ['c_v', 'u_mass'], index=0)\n",
        "    max_docs = st.slider(\"Jumlah dokumen untuk evaluasi (coherence)\", min_value=100, max_value=len(corpus), value=min(1000, len(corpus)), step=100)\n",
        "\n",
        "# Fungsi menghitung topic diversity\n",
        "def topic_diversity(model, topn=10):\n",
        "    topics = model.show_topics(num_topics=model.num_topics, num_words=topn, formatted=False)\n",
        "    all_words = [word for topic in topics for word, _ in topic[1]]\n",
        "    unique_words = set(all_words)\n",
        "    return len(unique_words) / (model.num_topics * topn)\n",
        "\n",
        "# Evaluasi dengan spinner\n",
        "with st.spinner(\"Sedang menghitung metrik evaluasi...\"):\n",
        "    # 1. Perplexity\n",
        "    perplexity = np.exp2(-lda_model.log_perplexity(corpus))\n",
        "\n",
        "    # 2. Coherence Score\n",
        "    sample_texts = data['tokens'][:max_docs]\n",
        "    sample_corpus = corpus[:max_docs]\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=sample_texts, dictionary=dictionary, coherence=coherence_type)\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # 3. Log Likelihood\n",
        "    log_likelihood = lda_model.log_perplexity(corpus) * sum(len(doc) for doc in corpus)\n",
        "\n",
        "    # 4. Topic Diversity\n",
        "    diversity = topic_diversity(lda_model)\n",
        "\n",
        "# Tampilkan hasil evaluasi\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"Perplexity\", f\"{perplexity:.2f}\")\n",
        "    st.metric(\"Coherence Score\", f\"{coherence_score:.2f} ({coherence_type})\")\n",
        "with col2:\n",
        "    st.metric(\"Topic Diversity\", f\"{diversity:.2f}\")\n",
        "    st.metric(\"Log Likelihood\", f\"{log_likelihood:.2f}\")\n",
        "\n",
        "# ====================== 7. Input Dataset Baru & Validasi ======================\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"📤 Validasi Dataset Baru\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Unggah file CSV untuk validasi\", type=[\"csv\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    new_data = pd.read_csv(uploaded_file)\n",
        "    if 'description' not in new_data.columns:\n",
        "        st.error(\"File harus memiliki kolom 'description'.\")\n",
        "    else:\n",
        "        # Preprocessing\n",
        "        def preprocess(text):\n",
        "            return [word for word in simple_preprocess(text, deacc=True) if word not in STOPWORDS]\n",
        "\n",
        "        new_data['tokens'] = new_data['description'].apply(preprocess)\n",
        "        new_corpus = [dictionary.doc2bow(text) for text in new_data['tokens']]\n",
        "        \n",
        "        if st.button(\"Validasi Dataset\"):\n",
        "            topic_results = []\n",
        "            for bow in new_corpus:\n",
        "                topic_dist = lda_model.get_document_topics(bow)\n",
        "                topic_results.append(sorted(topic_dist, key=lambda x: -x[1])[0][0])  # top topic\n",
        "            \n",
        "            # ✅ Tambahkan +1 agar topik dari 1–5\n",
        "            new_data['Topik'] = [topic + 1 for topic in topic_results]\n",
        "\n",
        "            st.success(\"Validasi selesai. Berikut hasil topiknya:\")\n",
        "            st.dataframe(new_data[['description', 'Topik']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t=2025-06-16T19:47:25+0700 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "t=2025-06-16T19:47:25+0700 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "t=2025-06-16T19:47:25+0700 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "t=2025-06-16T19:47:25+0700 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m public_url \u001b[38;5;241m=\u001b[39m \u001b[43mngrok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8501\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamlit app is live at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Sabrina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\ngrok.py:385\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[0;32m    381\u001b[0m _upgrade_legacy_params(pyngrok_config, options)\n\u001b[0;32m    383\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpening tunnel named: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 385\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapi_url\n\u001b[0;32m    387\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    389\u001b[0m tunnel \u001b[38;5;241m=\u001b[39m NgrokTunnel(api_request(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/tunnels\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    390\u001b[0m                                  timeout\u001b[38;5;241m=\u001b[39mpyngrok_config\u001b[38;5;241m.\u001b[39mrequest_timeout),\n\u001b[0;32m    391\u001b[0m                      pyngrok_config, api_url)\n",
            "File \u001b[1;32mc:\\Users\\Sabrina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\ngrok.py:203\u001b[0m, in \u001b[0;36mget_ngrok_process\u001b[1;34m(pyngrok_config)\u001b[0m\n\u001b[0;32m    199\u001b[0m     pyngrok_config \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget_default()\n\u001b[0;32m    201\u001b[0m install_ngrok(pyngrok_config)\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Sabrina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\process.py:270\u001b[0m, in \u001b[0;36mget_process\u001b[1;34m(pyngrok_config)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path):\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path]\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Sabrina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyngrok\\process.py:447\u001b[0m, in \u001b[0;36m_start_process\u001b[1;34m(pyngrok_config)\u001b[0m\n\u001b[0;32m    444\u001b[0m kill_process(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process\u001b[38;5;241m.\u001b[39mstartup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    448\u001b[0m                             ngrok_process\u001b[38;5;241m.\u001b[39mlogs,\n\u001b[0;32m    449\u001b[0m                             ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process was unable to start.\u001b[39m\u001b[38;5;124m\"\u001b[39m, ngrok_process\u001b[38;5;241m.\u001b[39mlogs)\n",
            "\u001b[1;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.port 8501\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "public_url = ngrok.connect(addr=8501)\n",
        "print(f\"Streamlit app is live at: {public_url}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
